<?xml version="1.0" encoding="utf-8" ?>
<!DOCTYPE article PUBLIC "-//NLM//DTD JATS (Z39.96) Journal Archiving and Interchange DTD v1.2 20190208//EN"
                  "JATS-archivearticle1.dtd">
<article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.2" article-type="other">
<front>
<journal-meta>
<journal-id></journal-id>
<journal-title-group>
</journal-title-group>
<issn></issn>
<publisher>
<publisher-name></publisher-name>
</publisher>
</journal-meta>
<article-meta>
<title-group>
<article-title>Challenges in automated DOI resolution</article-title>
</title-group>
<contrib-group>
<contrib contrib-type="author">
<string-name>Martin Fenner</string-name>
</contrib>
</contrib-group>
<pub-date pub-type="epub" iso-8601-date="2013-10-13">
<day>13</day>
<month>10</month>
<year>2013</year>
</pub-date>
</article-meta>
</front>
<body>
<p>Yesterday we created a set of roughly 10,000 DOIs for journal
articles published in 2011 or 2012. We used these DOIs as a reference
set in a
<ext-link ext-link-type="uri" xlink:href="http://almdatachallenge.eventbrite.com/">data
hackathon</ext-link> around article-level metrics/altmetrics - material
for another blog post.</p>
<p>The random DOis were generated using the
<ext-link ext-link-type="uri" xlink:href="http://random.labs.crossref.org/">CrossRef
RanDOIm service</ext-link>, with article titles fetched from the
<ext-link ext-link-type="uri" xlink:href="http://labs.crossref.org/openurl/">CrossRef
OpenURL API</ext-link>. We didn’t have time to properly parse the
publication date and only used the publication year. We used the
<monospace>crossref_r</monospace> and <monospace>crossref</monospace>
functions from the rOpenSci
<ext-link ext-link-type="uri" xlink:href="http://ropensci.github.io/rplos/">rplos
package</ext-link> (and some extra help from Scott Chamberlain) to
achieve this, the datasets were deposited to figshare and can be found
<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.6084/m9.figshare.821209">here</ext-link>
(2011) and
<ext-link ext-link-type="uri" xlink:href="http://dx.doi.org/10.6084/m9.figshare.821213">here</ext-link>
(2012).</p>
<p>The basic idea behind DOI names is summarized well in the
<ext-link ext-link-type="uri" xlink:href="http://en.wikipedia.org/wiki/Digital_object_identifier">Wikipedia
entry</ext-link>:</p>
<p specific-use="wrapper">
  <disp-quote>
    <p>A digital object identifier (DOI) is a character string (a
    “digital identifier”) used to uniquely identify an object such as an
    electronic document. Metadata about the object is stored in
    association with the DOI name and this metadata may include a
    location, such as a URL, where the object can be found. The DOI for
    a document is permanent, whereas its location and other metadata may
    change. Referring to an online document by its DOI provides more
    stable linking than simply referring to it by its URL, because if
    its URL changes, the publisher need only update the metadata for the
    DOI to link to the new URL.</p>
  </disp-quote>
</p>
<p>DOIs for journal articles should provide users with a URL specific
for that journal article. This URL could point to a digital copy of the
journal article in HTML or PDF format, or could point to a landing page
(with an abstract or other basic metadata) for journal articles that
require a subscription. This should work not only for humans using a web
browser, but also for automated services using command line tools such
as
<ext-link ext-link-type="uri" xlink:href="http://curl.haxx.se/">curl</ext-link>
as scientific infrastructure depends heavily on automation and computers
talking to each other. In our use case we want to find content linking
to a specific article, and as some services (e.g. social media) will use
the URL and not DOI of an article, we need to find out that URL.</p>
<p>Unfortunately it was difficult to find a URL for many DOIs in our
reference set using automated tools. All these DOIs resolve to URLs for
human users using a web browser, but for automated tools there are a
number of challenges:</p>
<sec id="requiring-a-cookie">
  <title>Requiring a cookie</title>
  <p>Some publishers require a cookie, and that can cause problems for
  automated tools. We can use the popular command line tool
  <monospace>curl</monospace> with the options <monospace>-L</monospace>
  to follow redirects and <monospace>-I</monospace> to only send the
  header (as we care about the location and not the content of the
  page).</p>
  <preformat>curl -I -L &quot;http://dx.doi.org/10.1080/13658816.2010.531020&quot;</preformat>
  <p>This command will lead us not to a page specific for that article,
  but to a “Cookie absent” page. You can work around this by having curl
  accept cookies:</p>
  <preformat>curl -I -L --cookie &quot;tmp&quot; &quot;http://dx.doi.org/10.1080/13658816.2010.531020&quot;</preformat>
  <p>Unfortunately not all tools do this. The way Facebook tracks likes,
  shares, comments, etc. is a prominent example.</p>
</sec>
<sec id="too-many-redirects">
  <title>Too many redirects</title>
  <p>Some DOIs never resolve using a HEAD request, and curl stops after
  50 redirects:</p>
  <preformat>curl -I -L &quot;http://dx.doi.org/10.1097/SLA.0b013e318235e525&quot;</preformat>
  <p>This error may relate to the “requiring a cookie” error above.</p>
</sec>
<sec id="method-not-allowed">
  <title>Method not allowed</title>
  <p>Some DOis HEAD requests result in a “405 Method Not Allowed” error.
  The reason is that the journal platform doesn’t accept the HEAD
  request, but wants a GET instead.</p>
  <preformat>curl -I -L &quot;http://dx.doi.org/10.1002/sam.10120&quot;</preformat>
  <p>The
  <ext-link ext-link-type="uri" xlink:href="http://www.w3.org/Protocols/rfc2616/rfc2616-sec9.html">HTTP
  1.1 protocol</ext-link> says about HEAD:</p>
  <p specific-use="wrapper">
    <disp-quote>
      <p>The HEAD method is identical to GET except that the server MUST
      NOT return a message-body in the response. … This method is often
      used for testing hypertext links for validity, accessibility, and
      recent modification.</p>
    </disp-quote>
  </p>
  <p>We can work around this error by using a GET request, which
  unfortunately creates extra overhead and is not the recommended way to
  obtain this kind of information.</p>
</sec>
<sec id="empty-reply-from-server">
  <title>Empty reply from server</title>
  <p>Some DOIs never resolve using a HEAD because curl reports “Empty
  reply from server” and we don’t get a HTTP 200 status code.</p>
  <preformat>curl -I -L &quot;http://dx.doi.org/10.1016/j.cca.2011.04.012&quot;</preformat>
  <p>You can again work around this by using the location information
  before the last redirect, but maybe resolving a DOI should not result
  in curl routinely throwing an error. It looks as if this error is
  related to “method not allowed”, as a GET request resolves to a
  landing page.</p>
  <p>This problem is not specific to the <monospace>curl</monospace>
  tool, we get exactly the same error with
  <monospace>wget</monospace>:</p>
  <preformat>wget -S --spider &quot;http://dx.doi.org/10.1016/j.cca.2011.04.012&quot;</preformat>
</sec>
<sec id="timeout-errors">
  <title>Timeout errors</title>
  <p>Some DOI resolutions resulted in timeout errors, but this was
  temporary and much less frequent than the errors above.</p>
</sec>
<sec id="resource-not-found">
  <title>Resource not found</title>
  <p>We didn’t specifically look into this error, which is a well-known
  problem with URLs. The DOI names we used were from 2011 and 2012, and
  it is known that
  <ext-link ext-link-type="uri" xlink:href="http://en.wikipedia.org/wiki/Link_rot">link
  rot</ext-link> is more common the older the resource is.</p>
</sec>
<sec id="content-negotiation">
  <title>Content negotiation</title>
  <p>As Karl Ward has pointed out in the comments there are other ways
  to get to the URL from the DOI name, e.g. using content
  negotiation:</p>
  <preformat>curl -LH &quot;Accept: application/vnd.crossref.unixref+xml&quot; &quot;http://dx.doi.org/10.1016/j.cca.2011.04.012&quot;</preformat>
  <p>The URL is stored in the <monospace>doi_data/resource</monospace>
  attribute. The URL stored there is unfortunately not always the final
  landing page for the article, e.g. for the DOI name used in the
  example above.</p>
</sec>
<sec id="conclusions">
  <title>Conclusions</title>
  <p>We created a reference set of 10,000 DOIs to collect metrics around
  them. The first conclusion from this exercise is that getting the URL
  for these articles is a challenge in many cases. This does not seem to
  relate to a permission problem for subscription content, but rather
  how the HTTP HEAD request is handled. Content negotiation is one
  alternative, but sometimes leads to different URLs for the landing
  page than where the user would get via the browser. We therefore have
  to rewrite our code to use GET requests and to better handle the
  scenarios above.</p>
  <p><italic>Update 10/13/13: Updated the title and the text to make it
  clear that I am not talking about DOIs that don’t resolve for human
  users, but rather about the problems automating this process using
  command-line tools.</italic></p>
</sec>
</body>
<back>
</back>
</article>
